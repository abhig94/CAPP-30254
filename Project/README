Docs:
	text files explaining apsects of the data
Paper:
	flow charts for the paper

Plots:
	Plots generated from exploring and analyzing the data.

categorical_summary.csv:
	summary statistics of categorical variables.

numeric_summary.csv:
	summary of numerical variables

notes.txt:
	notes from meeting with our professor, Ghani.

Pipeline:
	folder containing all functions to handle our data analysis and cleaning.

	Python files:
	data_cleaning:	file to clean the data
	cluster_muck: 
	cluster_pass:
	cluster_run:
	first_pass:		first pass to set up the data
	first_run:		first testing run of data for analysis.
	graphHelper:	helper functions for graphing
	handleData:		general file to help with handling the data
	macro_pass:		file with functions to set up and clean the survey and macro 					data.
	macro_run:		takes output from macro_pass and runs pipeline on compiled and  				cleaned macro data
	micro_world.csv: original, uncleaned survey data.
	mike.py:		testing stage region for various functions written by Mike
	old_files:		folder with discarded old code.
	pipe:			functions to run pipeline
	pipe_old:		old pipeline functions that were discarded
	revolutions_run: implements custom ensemble model on the data
	rfHandler:		fix for random forest models that kept old results and redid 					random forest results.
	sample_comparisions_run:  create sample run for comparision data
	true_run:		script with options to run pipeline on original, macro, or 						blind data with another option to run it with the custom 						ensemble or not.

(data was kept on dropbox due to size constraints on git)
Data Documentation:

General Folder contains:
Output: Output files from our code

	Within Output:
	Graphics - graphics created to explore the data
	Odds and Ends - Odds and Ends, test files
	Resuts: The resuts from running our pipeline on the data.
		Within Results:
		Weight Test - Results from within weight testing.

		File naming conventions are as follows for the output files are as follows. First, we have "best_clf" shows the best classifiers for each of the metrics we look at. "Comparison" compares the one classifier that get the best AUC across all of our metrics. Write results have all of of the results from our parameter sweep. The next part of the name describe what dataset we used: Macro, or original. Finally, the models that used our custom ensemble method are attached with "ensemble". Also included in this repository are the precision recall and AUC comparison plots we created from the data in this folder, named in a clear way.
		
	macro_var_names: Names of the macro variables
	macro_vars.xslx - the macroeconomic variables we sued.
	weights.csv - The output of the sample weights.
	x_clusterpass.csv - the cleaned data for running ensemble on our k means clusters.
	x_macro_data - the cleaned maco data features
	x_original_Data - the cleaned original data.
	y - the y variables.


Our various input variables are also included in this folder.